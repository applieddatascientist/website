"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[2569],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>h});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),d=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=d(e.components);return a.createElement(l.Provider,{value:t},e.children)},m="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=d(n),u=r,h=m["".concat(l,".").concat(u)]||m[u]||c[u]||i;return n?a.createElement(h,o(o({ref:t},p),{},{components:n})):a.createElement(h,o({ref:t},p))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[m]="string"==typeof e?e:r,o[1]=s;for(var d=2;d<i;d++)o[d]=n[d];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},7059:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var a=n(7462),r=(n(7294),n(3905));const i={sidebar_position:2},o="Forecasting Models",s={unversionedId:"forecasting/models",id:"forecasting/models",title:"Forecasting Models",description:"Let's delve into each of the forecasting methods and explore them in more depth:",source:"@site/docs/forecasting/models.md",sourceDirName:"forecasting",slug:"/forecasting/models",permalink:"/docs/forecasting/models",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/forecasting/models.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Basics",permalink:"/docs/forecasting/basics"},next:{title:"Python",permalink:"/docs/category/python"}},l={},d=[{value:"Moving Averages",id:"moving-averages",level:3},{value:"Exponential Smoothing",id:"exponential-smoothing",level:3},{value:"ARIMA (AutoRegressive Integrated Moving Average)",id:"arima-autoregressive-integrated-moving-average",level:3},{value:"Prophet",id:"prophet",level:3},{value:"Long Short-Term Memory (LSTM)",id:"long-short-term-memory-lstm",level:3},{value:"Ensemble Methods",id:"ensemble-methods",level:3}],p={toc:d},m="wrapper";function c(e){let{components:t,...n}=e;return(0,r.kt)(m,(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"forecasting-models"},"Forecasting Models"),(0,r.kt)("p",null,"Let's delve into each of the forecasting methods and explore them in more depth:"),(0,r.kt)("h3",{id:"moving-averages"},"Moving Averages"),(0,r.kt)("p",null,'Moving averages is a simple forecasting method that involves calculating the average of a fixed number of previous observations to forecast future values. The "moving" aspect refers to the fact that the window of observations used for calculating the average moves through the time series.'),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Simple Moving Average (SMA):"),"\nThe SMA assigns equal weights to all the observations within the window. It is calculated by summing up the values within the window and dividing by the window size. The size of the window determines how many past observations are considered in the calculation.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Weighted Moving Average (WMA):"),"\nWMA assigns different weights to the observations within the window. The weights can be assigned in a linear or exponential manner. Linearly weighted moving averages give more importance to recent observations, while exponentially weighted moving averages assign decreasing weights as the observations move further into the past."))),(0,r.kt)("p",null,"Moving averages are particularly useful for smoothing out short-term fluctuations and identifying underlying trends in the data. However, they tend to lag behind sudden changes or sharp shifts in the time series."),(0,r.kt)("h3",{id:"exponential-smoothing"},"Exponential Smoothing"),(0,r.kt)("p",null,"Exponential smoothing is a popular method that assigns exponentially decreasing weights to past observations. The key idea is to give more importance to recent data while gradually decreasing the influence of older observations."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Simple Exponential Smoothing (SES):"),"\nSES is suitable for time series data with no trend or seasonality. It involves calculating a smoothed value (also known as the level) based on the previous smoothed value and the most recent observation. The level is updated by combining the previous level and the error in the previous forecast.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Holt's Linear Exponential Smoothing (Double Exponential Smoothing):"),"\nHolt's linear method extends SES to capture trends in the data. In addition to the level, it also estimates the trend component of the time series. The trend is updated using the previous trend value and the error in the previous forecast.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Holt-Winters' Exponential Smoothing (Triple Exponential Smoothing):"),"\nHolt-Winters' method incorporates both level and trend as well as seasonality components. It is suitable for time series data with both trend and seasonality. The seasonality component captures the repeating patterns at fixed intervals. The method includes equations for updating the level, trend, and seasonal components."))),(0,r.kt)("h3",{id:"arima-autoregressive-integrated-moving-average"},"ARIMA (AutoRegressive Integrated Moving Average)"),(0,r.kt)("p",null,"ARIMA is a powerful and widely used forecasting method that considers the autocorrelation, differencing, and moving average components of a time series. It requires the data to be stationary, meaning it should have constant mean and variance."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"AutoRegressive (AR) Component:"),'\nThe AR component models the relationship between an observation and a linear combination of its lagged values. The "p" parameter determines the number of lagged values to include in the model.')),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Integrated (I) Component:"),"\nThe I component deals with differencing, which helps transform non-stationary data into stationary data. Differencing involves subtracting each observation from its previous observation, removing trends or seasonality.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Moving Average (MA) Component:"),'\nThe MA component models the dependency between an observation and a linear combination of past error terms. The "q" parameter determines the number of lagged error terms to include in the model.'))),(0,r.kt)("p",null,"ARIMA models are widely used due to their flexibility in capturing various time series patterns. However, selecting appropriate values for the ARIMA parameters (p, d, q) can be challenging and often requires analyzing the autocorrelation and partial autocorrelation plots of the data."),(0,r.kt)("h3",{id:"prophet"},"Prophet"),(0,r.kt)("p",null,"Prophet is an open-source forecasting library developed by Facebook. It"),(0,r.kt)("p",null," is designed to handle time series data with trends, seasonality, and holidays."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Prophet utilizes a decomposable time series model that captures the trend using a piecewise linear model and seasonality using Fourier series. It also incorporates additional features such as holidays, which can be specified to account for special events that impact the time series.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Prophet provides a user-friendly interface and automates several steps in the forecasting process, making it accessible to both beginners and experts. It can handle missing values, outliers, and changes in trend, making it robust for various forecasting scenarios."))),(0,r.kt)("h3",{id:"long-short-term-memory-lstm"},"Long Short-Term Memory (LSTM)"),(0,r.kt)("p",null,"LSTMs are designed to overcome the vanishing gradient problem of traditional RNNs. They introduce specialized memory cells that allow information to be stored and retrieved over longer time intervals. LSTMs have a more complex architecture compared to traditional RNNs, consisting of input gates, output gates, and forget gates that control the flow of information."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Sequence-to-Sequence Forecasting with LSTMs:"),"\nIn time series forecasting, the input sequence consists of past observations, and the output sequence is the forecasted future values. LSTM models can be trained in a sequence-to-sequence manner, where the input sequence is fed into the LSTM, and the output sequence is generated by the LSTM's predictions.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Training and Tuning LSTMs for Forecasting:"),"\nTo train an LSTM model for time series forecasting, historical data is divided into input sequences (past observations) and corresponding output sequences (future values). The model is trained to minimize the difference between its predicted outputs and the true future values. Hyperparameters such as the number of LSTM layers, hidden units, and learning rate can be tuned to improve model performance.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Multivariate Time Series Forecasting with LSTMs:"),"\nLSTMs can handle multivariate time series data, where multiple variables are used to predict future values. For example, in stock market forecasting, input features like historical prices, trading volumes, and news sentiment can be incorporated as input variables to improve forecasting accuracy."))),(0,r.kt)("p",null,"LSTM models have gained popularity in time series forecasting due to their ability to capture complex patterns, handle long-term dependencies, and work well with both univariate and multivariate data. They are often implemented using deep learning frameworks such as TensorFlow or PyTorch, which provide high-level APIs for building and training LSTM models."),(0,r.kt)("h3",{id:"ensemble-methods"},"Ensemble Methods"),(0,r.kt)("p",null,"Ensemble methods involve combining multiple forecasting models to improve accuracy and reduce errors. Two popular ensemble methods are:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Averaging and Weighted Averaging:")," This approach combines forecasts from multiple models by taking the average of their predicted values. Weighted averaging assigns different weights to each model based on their performance or expertise in specific areas.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Bagging and Boosting:")," Bagging and boosting are ensemble techniques that use multiple models trained on different subsets of data or with different weights. Bagging, for example, creates an ensemble by training each model on a bootstrap sample of the original data. Boosting, on the other hand, trains models sequentially, with each model focused on correcting the errors made by the previous model."))),(0,r.kt)("p",null,"Ensemble methods help mitigate the limitations of individual models and capture a broader range of patterns in the data."),(0,r.kt)("p",null,"These are just some of the forecasting methods available, and each has its strengths and weaknesses. Depending on the characteristics of your time series data and the forecasting requirements, you can choose the most suitable method or even combine multiple methods to achieve better accuracy."))}c.isMDXComponent=!0}}]);